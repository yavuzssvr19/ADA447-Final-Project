{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1913658,"sourceType":"datasetVersion","datasetId":1036526},{"sourceId":2356471,"sourceType":"datasetVersion","datasetId":1422991},{"sourceId":442363,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":359335,"modelId":380635}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom shutil import copy2\n\n# Convert a BGR image to LAB and normalize each channel\ndef convert_to_lab_channels(color_img_bgr):\n    img_lab = cv2.cvtColor(color_img_bgr, cv2.COLOR_BGR2LAB).astype(np.float32)\n    L = img_lab[:,:,0] / 255.0\n    a = (img_lab[:,:,1] - 128.0) / 127.0\n    b = (img_lab[:,:,2] - 128.0) / 127.0\n    return L.astype(np.float32), a.astype(np.float32), b.astype(np.float32)\n\n# Dataset paths\ndataset1_color = '/kaggle/input/landscape-image-colorization/landscape Images/color'\ndataset1_gray  = '/kaggle/input/landscape-image-colorization/landscape Images/gray'\n\ndataset2_base = '/kaggle/input/image-colorization-dataset/data'\ndataset2_train_color = os.path.join(dataset2_base, \"train_color\")\ndataset2_train_gray  = os.path.join(dataset2_base, \"train_black\")\ndataset2_test_color  = os.path.join(dataset2_base, \"test_color\")\ndataset2_test_gray   = os.path.join(dataset2_base, \"test_black\")\n\noutput_base = '/kaggle/working/landscape_data'\n\n# Create output directory structure\nsplits = ['train', 'valid', 'test']\nfor split in splits:\n    for subdir in ['color', 'grayscale', 'lab_L', 'lab_ab']:\n        os.makedirs(os.path.join(output_base, split, subdir), exist_ok=True)\n\n# Process one image pair: copy images and save L/ab channels as .npy\ndef process_pair(color_path, gray_path, filename, split):\n    copy2(color_path, os.path.join(output_base, split, 'color', filename))\n    copy2(gray_path, os.path.join(output_base, split, 'grayscale', filename))\n\n    img_color_bgr = cv2.imread(color_path)\n    L, a, b = convert_to_lab_channels(img_color_bgr)\n\n    np.save(os.path.join(output_base, split, 'lab_L', filename.replace('.jpg', '.npy')), L)\n    ab = np.stack([a, b], axis=0)\n    np.save(os.path.join(output_base, split, 'lab_ab', filename.replace('.jpg', '.npy')), ab)\n\n# Split dataset 1 (landscape) into train/valid/test\nall_files_1 = sorted(os.listdir(dataset1_color))\nnp.random.seed(42)\nnp.random.shuffle(all_files_1)\n\nn1 = len(all_files_1)\nn1_train, n1_valid = int(0.8 * n1), int(0.1 * n1)\n\nsplits1 = {\n    'train': all_files_1[:n1_train],\n    'valid': all_files_1[n1_train:n1_train + n1_valid],\n    'test':  all_files_1[n1_train + n1_valid:]\n}\n\nfor split, files in splits1.items():\n    for fname in files:\n        process_pair(\n            os.path.join(dataset1_color, fname),\n            os.path.join(dataset1_gray, fname),\n            fname, split\n        )\n\n# Split dataset 2 (custom dataset) into train/valid\nall_files_2_train = sorted(os.listdir(dataset2_train_color))\nnp.random.seed(42)\nnp.random.shuffle(all_files_2_train)\n\nn2 = len(all_files_2_train)\nn2_train = int(0.8 * n2)\n\nfiles2_train = all_files_2_train[:n2_train]\nfiles2_valid = all_files_2_train[n2_train:]\n\n# Process train set from dataset 2\nfor fname in files2_train:\n    process_pair(\n        os.path.join(dataset2_train_color, fname),\n        os.path.join(dataset2_train_gray, fname),\n        fname, 'train'\n    )\n\n# Process valid set from dataset 2\nfor fname in files2_valid:\n    process_pair(\n        os.path.join(dataset2_train_color, fname),\n        os.path.join(dataset2_train_gray, fname),\n        fname, 'valid'\n    )\n\n# Process test set from dataset 2\nfiles2_test = sorted(os.listdir(dataset2_test_color))\nfor fname in files2_test:\n    process_pair(\n        os.path.join(dataset2_test_color, fname),\n        os.path.join(dataset2_test_gray, fname),\n        fname, 'test'\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom collections import defaultdict\nimport pandas as pd\n# For obeserving the structure of file\n\nbase_path = '/kaggle/working/landscape_data'\n\n\nsplits = ['train', 'valid', 'test']\nsubfolders = ['color', 'grayscale', 'lab_L', 'lab_ab']\n\n\nfile_counts = defaultdict(dict)\n\n\nfor split in splits:\n    for sub in subfolders:\n        folder_path = os.path.join(base_path, split, sub)\n        if os.path.exists(folder_path):\n            num_files = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n        else:\n            num_files = 0\n        file_counts[split][sub] = num_files\n\n\ndf = pd.DataFrame(file_counts).T\nprint(df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# needed libraries\nimport os\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.models.segmentation import deeplabv3_resnet50\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T12:07:06.277991Z","iopub.execute_input":"2025-06-20T12:07:06.278296Z","iopub.status.idle":"2025-06-20T12:07:14.855205Z","shell.execute_reply.started":"2025-06-20T12:07:06.278279Z","shell.execute_reply":"2025-06-20T12:07:14.854610Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install transformers timm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If you have a CUDA capable GPU, use it, otherwise switch to CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import the DeepLabV3 model with a ResNet-101 backbone from torchvision\nfrom torchvision.models.segmentation import deeplabv3_resnet101\n\n# Load the pretrained model (trained on ImageNet + COCO) and move it to the appropriate device (GPU if available)\ndeeplab = deeplabv3_resnet101(pretrained=True).to(device)\n\n# Set the model to evaluation mode ‚Äî disables layers like dropout and batchnorm updates\ndeeplab.eval()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to generate a segmentation mask using the DeepLab model\ndef get_segmentation_mask(image):\n    # Define preprocessing transformations: convert to PIL, resize, normalize\n    transform = transforms.Compose([\n        transforms.ToPILImage(),               # Convert NumPy array to PIL image\n        transforms.Resize((256, 256)),         # Resize to 256x256 (model input size)\n        transforms.ToTensor(),                 # Convert to tensor and scale to [0,1]\n        transforms.Normalize(                  # Normalize using ImageNet mean and std\n            mean=[0.485, 0.456, 0.406], \n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n    \n    # Apply transformations and add batch dimension\n    input_tensor = transform(image).unsqueeze(0).to(device)\n\n    # Run inference with the segmentation model (no gradient calculation)\n    with torch.no_grad():\n        output = deeplab(input_tensor)['out'][0]  # Get raw segmentation logits\n\n    # Convert model output to segmentation mask (take argmax over class dimension)\n    seg_mask = output.argmax(0).cpu().numpy()\n\n    return seg_mask  # Return as NumPy array (H, W) with class indices\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Custom dataset class for fast loading of colorization data with segmentation support\nclass ColorizationDatasetFast(Dataset):\n    def __init__(self, root_dir, split='train', target_size=(256, 256)):\n        # Paths for L channel, ab channels, and grayscale images\n        self.l_path = os.path.join(root_dir, split, 'lab_L')\n        self.ab_path = os.path.join(root_dir, split, 'lab_ab')\n        self.gray_path = os.path.join(root_dir, split, 'grayscale')  # only used for filenames\n        self.size = target_size\n        self.files = sorted(os.listdir(self.gray_path))  # list of image filenames\n\n    def __len__(self):\n        # Return total number of samples\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        # Replace file extension to match .npy format\n        filename = self.files[idx].replace('.jpg', '.npy')\n\n        # Load L and ab channels from .npy files\n        l = np.load(os.path.join(self.l_path, filename))  # shape: (H, W)\n        ab = np.load(os.path.join(self.ab_path, filename))  # shape: (2, H, W)\n\n        # Resize L and ab channels to target size\n        l = cv2.resize(l, self.size, interpolation=cv2.INTER_LINEAR)  # shape: (H, W)\n        ab_resized = np.stack([\n            cv2.resize(ab[0], self.size, interpolation=cv2.INTER_LINEAR),\n            cv2.resize(ab[1], self.size, interpolation=cv2.INTER_LINEAR)\n        ], axis=0)  # shape: (2, H, W)\n\n        # Convert to torch tensors\n        l = torch.from_numpy(l).unsqueeze(0).float()  # shape: (1, H, W)\n        ab = torch.from_numpy(ab_resized).float()     # shape: (2, H, W)\n\n        # Load corresponding grayscale image for segmentation (convert to RGB)\n        rgb_image = cv2.imread(os.path.join(self.gray_path, self.files[idx]))  # BGR image\n        rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)  # convert to RGB\n        rgb_image = cv2.resize(rgb_image, self.size)  # resize to target size\n\n        # Generate segmentation mask from RGB image\n        seg_mask = get_segmentation_mask_from_np(rgb_image)  # shape: (H, W)\n\n        # Convert segmentation mask to tensor\n        seg_mask_tensor = torch.from_numpy(seg_mask).long()  # shape: (H, W)\n\n        # Return L channel, ab channels, and segmentation mask\n        return l, ab, seg_mask_tensor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install efficientnet_pytorch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\n# A U-Net style architecture with EfficientNet-B0 encoder and segmentation mask fusion\nclass EfficientUNetWithSeg(nn.Module):\n    def __init__(self, n_classes=313):\n        super().__init__()\n\n        # Load pretrained EfficientNet-B0 as encoder\n        self.encoder = EfficientNet.from_pretrained('efficientnet-b0')\n\n        # Convert 2 input channels (L channel + segmentation embedding) into 3 for EfficientNet\n        self.input_conv = nn.Conv2d(2, 3, kernel_size=1)\n\n        # Extract intermediate encoder blocks (EfficientNet layers)\n        self.enc1 = nn.Sequential(\n            self.encoder._conv_stem,\n            self.encoder._bn0,\n            self.encoder._swish\n        )  # Output: [B, 32, H/2, W/2]\n\n        self.enc2 = nn.Sequential(*self.encoder._blocks[0:2])   # Output: [B, 24, H/4, W/4]\n        self.enc3 = nn.Sequential(*self.encoder._blocks[2:4])   # Output: [B, 40, H/8, W/8]\n        self.enc4 = nn.Sequential(*self.encoder._blocks[4:10])  # Output: [B, 80, H/16, W/16]\n        self.enc5 = nn.Sequential(*self.encoder._blocks[10:])   # Output: [B, 112, H/32, W/32]\n\n        # U-Net style decoder blocks with upsampling\n        self.up4 = self._up_block(320, 112)  # Skip connection with enc4\n        self.up3 = self._up_block(112, 40)   # Skip connection with enc3\n        self.up2 = self._up_block(40, 24)    # Skip connection with enc2\n        self.up1 = self._up_block(24, 32)    # Skip connection with enc1\n\n        # Segmentation mask embedding: maps class index to 1D embedding per pixel\n        self.seg_embed = nn.Embedding(21, 1)  # Output: [B, 1, H, W]\n\n        # Final convolution: predict ab channels (2 outputs per pixel)\n        self.final_conv = nn.Conv2d(32, 2, kernel_size=1)\n\n        # Final upsampling to match input size\n        self.upsample_final = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n\n    def _up_block(self, in_ch, out_ch):\n        # A basic upsampling block using transposed convolution followed by ReLU\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, l, seg_mask):\n        \"\"\"\n        Forward pass through the model.\n\n        Args:\n        l         -- Grayscale L channel input tensor, shape: [B, 1, H, W]\n        seg_mask  -- Segmentation mask tensor with class IDs, shape: [B, H, W]\n\n        Returns:\n        ab_pred   -- Predicted ab channels, shape: [B, 2, H, W]\n        \"\"\"\n\n        # Embed segmentation mask into continuous values\n        seg_emb = self.seg_embed(seg_mask.long())      # [B, H, W, 1]\n        seg_emb = seg_emb.permute(0, 3, 1, 2)          # [B, 1, H, W]\n\n        # Concatenate L channel and segmentation embedding\n        x = torch.cat([l, seg_emb], dim=1)             # [B, 2, H, W]\n        x = self.input_conv(x)                         # [B, 3, H, W]\n\n        # Encoder path\n        x1 = self.enc1(x)  # Output after stem\n        x2 = self.enc2(x1)\n        x3 = self.enc3(x2)\n        x4 = self.enc4(x3)\n        x5 = self.enc5(x4)\n\n        # Decoder path with skip connections\n        u4 = self.up4(x5) + x4\n        u3 = self.up3(u4) + x3\n        u2 = self.up2(u3) + x2\n        u1 = self.up1(u2) + x1\n\n        # Final convolution to get 2-channel ab prediction\n        out = self.final_conv(u1)\n        out = self.upsample_final(out)  # Upsample to original resolution\n\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Initialize model and move to GPU/CPU\nmodel = EfficientUNetWithSeg().to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.L1Loss()  # Mean Absolute Error (MAE) for regression\n\n# Learning rate scheduler: reduces LR when validation loss plateaus\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n)\n\n# Early stopping setup\nearly_stop_patience = 5\nbest_val_loss = float('inf')\nearly_stop_counter = 0\n\n# Load training and validation datasets\ntrain_dataset = ColorizationDatasetFast('/kaggle/working/landscape_data', split='train')\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\nvalid_dataset = ColorizationDatasetFast('/kaggle/working/landscape_data', split='valid')\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n\n# Training loop\nfor epoch in range(50):  # Max 50 epochs, early stopping will halt earlier if needed\n    model.train()\n    total_train_loss = 0.0\n\n    # Training step\n    for l, ab, seg in tqdm(train_loader, desc=f\"[Epoch {epoch+1:02d}] Training\"):\n        l, ab, seg = l.to(device), ab.to(device), seg.to(device)\n        output_ab = model(l, seg)  # Forward pass\n\n        loss = criterion(output_ab, ab)  # Compute L1 loss\n        optimizer.zero_grad()\n        loss.backward()                 # Backpropagation\n        optimizer.step()               # Update weights\n\n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    # Validation step\n    model.eval()\n    total_val_loss = 0.0\n    with torch.no_grad():\n        for l_val, ab_val, seg_val in valid_loader:\n            l_val, ab_val, seg_val = l_val.to(device), ab_val.to(device), seg_val.to(device)\n            output_val = model(l_val, seg_val)  # Forward pass on validation set\n\n            val_loss = criterion(output_val, ab_val)  # Compute validation loss\n            total_val_loss += val_loss.item()\n\n    avg_val_loss = total_val_loss / len(valid_loader)\n\n    # Step the learning rate scheduler\n    scheduler.step(avg_val_loss)\n\n    # Print epoch summary\n    print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n    # Early stopping logic: save model if improved\n    if avg_val_loss < best_val_loss - 1e-4:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), '/kaggle/working/best_model_earlystop.pth')  # Save best model\n        early_stop_counter = 0\n    else:\n        early_stop_counter += 1\n        print(f\"  üî∏ Early Stop Counter: {early_stop_counter}/{early_stop_patience}\")\n        if early_stop_counter >= early_stop_patience:\n            print(\"‚õîÔ∏è Early stopping triggered.\")\n            break  # Stop training\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test dataset from the specified path\ntest_dataset = ColorizationDatasetFast('/kaggle/working/landscape_data', split='test')\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n# Function to convert L and ab channels back to RGB image\ndef lab_to_rgb(L, ab):\n    # If L has shape (1, H, W), squeeze the batch dimension\n    if len(L.shape) == 3:\n        L = L[0]\n\n    # Denormalize L channel from [0, 1] to [0, 255]\n    L = (L * 255.0).astype(np.uint8)\n\n    # Denormalize a and b channels from [-1, 1] to [0, 255]\n    a = (ab[0] * 127.0 + 128).astype(np.uint8)\n    b = (ab[1] * 127.0 + 128).astype(np.uint8)\n\n    # Stack the three LAB channels together\n    lab = np.stack([L, a, b], axis=2)  # Shape: (H, W, 3)\n\n    # Convert LAB image to RGB using OpenCV\n    rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n    return rgb\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.metrics import structural_similarity as compare_ssim\nfrom skimage.metrics import peak_signal_noise_ratio\n\n# Initialize metric accumulators\ntotal_ssim = 0.0\ntotal_psnr = 0.0\ncount = 0\n\n# Load the trained model and move it to the appropriate device\nmodel = EfficientUNetWithSeg()\nmodel.load_state_dict(torch.load('/kaggle/input/bestt/pytorch/default/1/best_model_earlystop_BESTMODEL.pth', map_location=device))\nmodel.to(device)\nmodel.eval()  # Set model to evaluation mode\n\n# Disable gradient computation for evaluation\nwith torch.no_grad():\n    for l, ab, seg in test_loader:\n        # Move data to device (GPU or CPU)\n        l = l.to(device)\n        ab = ab.to(device)\n        seg = seg.to(device)\n\n        # Predict ab channels using the model\n        output_ab = model(l, seg)\n\n        # Convert tensors to numpy arrays for metric computation\n        l_np = l[0].cpu().numpy()\n        ab_true_np = ab[0].cpu().numpy()\n        ab_pred_np = output_ab[0].cpu().numpy()\n\n        # Convert both ground truth and predicted LAB to RGB\n        rgb_true = lab_to_rgb(l_np, ab_true_np)\n        rgb_pred = lab_to_rgb(l_np, ab_pred_np)\n\n        # Compute SSIM and PSNR between predicted and true RGB images\n        ssim_score = compare_ssim(rgb_true, rgb_pred, channel_axis=2, data_range=255)\n        psnr_score = peak_signal_noise_ratio(rgb_true, rgb_pred, data_range=255)\n\n        # Accumulate metrics\n        total_ssim += ssim_score\n        total_psnr += psnr_score\n        count += 1\n\n# Compute average metrics over the entire test set\navg_ssim = total_ssim / count\navg_psnr = total_psnr / count\n\n# Print final evaluation results\nprint(f\"‚úÖ Final Evaluation on Test Set:\")\nprint(f\"‚Üí Average SSIM : {avg_ssim:.4f}\")\nprint(f\"‚Üí Average PSNR : {avg_psnr:.2f} dB\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\ndef get_segmentation_mask_from_np(rgb_np):\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n    input_tensor = transform(rgb_np).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = deeplab(input_tensor)['out'][0]\n    seg_mask = output.argmax(0).cpu().numpy()\n    return seg_mask","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Load the pretrained model and set to evaluation mode\nmodel = EfficientUNetWithSeg().to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/input/bestt/pytorch/default/1/best_model_earlystop_BESTMODEL.pth\", map_location=device))\nmodel.eval()\n\n# 7. Main image colorization function\ndef colorize_image(gray_img_pil, mode, file_format):\n    # Normalize file format names (e.g., jpg -> JPEG)\n    if file_format.upper() == \"JPG\":\n        file_format = \"JPEG\"\n    elif file_format.upper() == \"WEBP\":\n        file_format = \"WEBP\"\n    elif file_format.upper() == \"TIFF\":\n        file_format = \"TIFF\"\n\n    # 1. Convert grayscale image (PIL) to NumPy array (shape: H x W)\n    gray_np_original = np.array(gray_img_pil.convert(\"L\"))  # Grayscale (H, W)\n    orig_h, orig_w = gray_np_original.shape  # Store original resolution\n\n    # 2. Resize to 256x256 and normalize for model input\n    gray_resized = cv2.resize(gray_np_original, (256, 256)) / 255.0\n    L_tensor = torch.tensor(gray_resized).unsqueeze(0).unsqueeze(0).float().to(device)  # (1, 1, 256, 256)\n\n    # 3. Create fake RGB from grayscale for segmentation mask\n    rgb_simulated = cv2.cvtColor(gray_np_original, cv2.COLOR_GRAY2RGB)\n    rgb_resized = cv2.resize(rgb_simulated, (256, 256))\n    seg_mask = get_segmentation_mask_from_np(rgb_resized)\n    seg_tensor = torch.tensor(seg_mask).unsqueeze(0).to(device)  # (1, 256, 256)\n\n    # 4. Predict ab color channels from model\n    with torch.no_grad():\n        ab_pred = model(L_tensor, seg_tensor)\n    ab_pred_np = ab_pred[0].cpu().numpy()  # (2, 256, 256)\n\n    # 5. Resize ab\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with gr.Blocks(theme=\"soft\") as demo:\n    gr.Markdown(\"## üé® AI-Powered Image Colorization\")\n    gr.Markdown(\"Colorize black-and-white images using a segmentation-assisted EfficientUNet model.\")\n\n    # 1. Image upload\n    input_image = gr.Image(label=\"üñºÔ∏è Upload Grayscale Image\", type=\"pil\")\n\n    # 2. Settings (mode + format)\n    with gr.Row():\n        mode = gr.Radio([\"Basic\", \"Advanced\"], value=\"Basic\", label=\"üß≠ Mode\")\n        file_format = gr.Radio([\"PNG\", \"JPG\", \"WEBP\", \"TIFF\"], value=\"PNG\", label=\"üóÇÔ∏è Output Format\")\n\n    # 3. Button\n    run_button = gr.Button(\"üöÄ Colorize\")\n\n    # 4. Gallery\n    output_gallery = gr.Gallery(label=\"üé¨ Before and After\", columns=2, height=300)\n\n    # 5. Download\n    download_button = gr.File(label=\"‚¨á Download Colorized Image\")\n\n    # Function\n    def process_wrapper(img, mode, fmt):\n        gallery, path = colorize_image(img, mode, fmt)\n        return gallery, path\n\n    run_button.click(fn=process_wrapper,\n                     inputs=[input_image, mode, file_format],\n                     outputs=[output_gallery, download_button])\n\ndemo.launch(share=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}